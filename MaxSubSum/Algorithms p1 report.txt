Algorithms
Practical 1: Maximum subsequence sum

Author 1: Martin do Rio Rico          | Login 1: martin.dorio
Author 2: Rodrigo Naranjo Gonzalez    | Login 2: r.naranjo
Author 3: Guillermo Fernandez Sanchez | Login 1: guillermo.fernandezs

We have two algorithms implemented which calculate the maximum subsequence sum from a given array of an n number of integers,
this is the sequence out of every possible subsequence in which the sum is the greater.

We have implemented the two algorithms and tested them with the sample of predetermined numbers included in the practice
and randomly generated arrays to check their proper performance.

After ensuring its correct operation, our objective was to check its complexity and measure their efficiency
to compare them to see which one is faster.

We have run the algorithms with random vectors of different sizes (n=500, 1000, 2000, 4000, 8000, 16000, 32000)
following a geometric progression (*2) and we got the execution time through a function after which
we performed an empirical verification of the complexity of the algorithms.

All the times are measured in microseconds (ms).

The executions times were calculated in a computer of the following characteristics:

     Os: Zorin OS 16 x86_64
    CPU: Intel i5-10400F (12) @ 4.300GHz
 Memory: 15946MiB

Execution times of the algorithm MaxSubSum 1 with different values of n:

       n              t(n)        t(n)/n^1.8        t(n)/n^2.0        t(n)/n^2.2
(*)  500           249.340          0.003457          0.000997          0.000288
    1000           992.000          0.003949          0.000992          0.000249
    2000          3907.000          0.004467          0.000977          0.000214
    4000         15714.000          0.005159          0.000982          0.000187
    8000         62607.000          0.005903          0.000978          0.000162
   16000        250880.000          0.006793          0.000980          0.000141
   32000       1005643.000          0.007819          0.000982          0.000123

n^1.8: underestimated bound
n^2.0: tight bound
n^2.2: overestimated bound

* When the execution time measured is smaller than 500 microseconds we consider that the figure is not significant enough.
  To solve this we run the algorithm 1000 times and then using the execution time obtained we can guarantee that it is a reliable result.
  This incident will be marked with an asterisk.

After running the program we can observe the following...

- The underestimated bound is constantly growing as we increase n so we can deduce it tends to infinity,
  while the overestimated bound is constantly decreasing, therefore it tends to zero.

- The tight bound tends to a constant contained in the interval between 0.00100 and 0.00097.

- The algorithm follows a quadratic growth, and its computational complexity is n^2.


Execution times of the algorithm MaxSubSum 2 with different values of n:

        n              t(n)        t(n)/n^0.8        t(n)/n^1.0        t(n)/n^1.2
(*)   500             1.170          0.008110          0.002340          0.000675
(*)  1000             2.230          0.008878          0.002230          0.000560
(*)  2000             4.430          0.010129          0.002215          0.000484
(*)  4000             8.150          0.010703          0.002038          0.000388
(*)  8000            17.380          0.013109          0.002172          0.000360
(*) 16000            33.740          0.014617          0.002109          0.000304
(*) 32000            64.590          0.016071          0.002018          0.000254

n^0.8: underestimated bound
n^1.0: tight bound
n^1.2: overestimated bound

* When the execution time measured is smaller than 500 microseconds we consider that the figure is not significant enough.
  To solve this we run the algorithm 1000 times and then using the execution time obtained we can guarantee that it is a reliable result.
  This incident will be marked with an asterisk.

After running the program we can observe the following...

- The underestimated bound is constantly growing as we increase n so we can deduce it tends to infinity,
  while the overestimated bound is constantly decreasing, therefore it tends to zero.

- The tight bound tends to a constant contained in the interval between 0.00240 and 0.00200.

- The algorithm follows a linear growth, and its computational complexity is n^1.


Finally, after running and measuring both algorithms we conclude that the second algorithm is faster and more efficient
since it follows a linear growth while the first one follows a quadratic one.
